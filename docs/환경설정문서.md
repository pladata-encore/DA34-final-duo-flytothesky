## 환경 구성

multi-modal

```sh
pip install -r requirements-multi-modal.txt
```

chat-qa

```sh
pip install -r requirements-chat-qa.txt
```

## Multi Modal App (gradio)

blah blah blah blah

```
```

## Chat QA App (open-web-ui)

### Ollama / Local LLM 설치

- 플랫폼에 맞게 Ollama 설치: <https://ollama.com/download>
- `model/README.md`를 참고하여 로컬 모델 Load

### Docker 실행

- open web ui

```sh
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

- pipelines

```sh
docker run -d -p 9099:9099 --add-host=host.docker.internal:host-gateway -v pipelines:/app/pipelines --name pipelines --restart always ghcr.io/open-webui/pipelines:main
```